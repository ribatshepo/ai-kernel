# Comprehensive monitoring dashboards for Istio Service Mesh
# Includes PrometheusRule for alerts and ServiceMonitor for scraping

---
# PrometheusRule for Istio alerting
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: istio-alerts
  namespace: aikernel-monitoring
  labels:
    prometheus: istio
    role: alert-rules
spec:
  groups:
  # Control Plane Alerts
  - name: istio.control-plane.rules
    interval: 30s
    rules:
    - alert: IstiodDown
      expr: up{job="pilot"} == 0
      for: 5m
      labels:
        severity: critical
        component: control-plane
      annotations:
        summary: "Istiod control plane is down"
        description: "Istiod (Pilot) in namespace {{ $labels.namespace }} has been down for more than 5 minutes"
        runbook_url: "https://istio.io/latest/docs/ops/diagnostic-tools/"

    - alert: IstiodHighCPU
      expr: rate(container_cpu_usage_seconds_total{pod=~"istiod-.*"}[5m]) > 0.8
      for: 10m
      labels:
        severity: warning
        component: control-plane
      annotations:
        summary: "Istiod high CPU usage"
        description: "Istiod pod {{ $labels.pod }} CPU usage is above 80% for 10 minutes"

    - alert: IstiodHighMemory
      expr: container_memory_usage_bytes{pod=~"istiod-.*"} / container_spec_memory_limit_bytes{pod=~"istiod-.*"} > 0.85
      for: 10m
      labels:
        severity: warning
        component: control-plane
      annotations:
        summary: "Istiod high memory usage"
        description: "Istiod pod {{ $labels.pod }} memory usage is above 85% for 10 minutes"

    - alert: IstiodConfigPushErrors
      expr: rate(pilot_xds_push_errors[5m]) > 0.1
      for: 5m
      labels:
        severity: warning
        component: control-plane
      annotations:
        summary: "High rate of configuration push errors"
        description: "Istiod is experiencing {{ $value }} config push errors per second"

  # Data Plane Alerts
  - name: istio.data-plane.rules
    interval: 30s
    rules:
    - alert: HighGlobalErrorRate
      expr: |
        sum(rate(istio_requests_total{reporter="source", response_code=~"5.*"}[5m]))
        /
        sum(rate(istio_requests_total{reporter="source"}[5m]))
        > 0.05
      for: 5m
      labels:
        severity: critical
        component: data-plane
      annotations:
        summary: "High global error rate detected"
        description: "Global error rate is {{ $value | humanizePercentage }} (threshold: 5%)"

    - alert: HighServiceErrorRate
      expr: |
        sum(rate(istio_requests_total{reporter="destination", response_code=~"5.*"}[5m])) by (destination_service_name, destination_workload_namespace)
        /
        sum(rate(istio_requests_total{reporter="destination"}[5m])) by (destination_service_name, destination_workload_namespace)
        > 0.05
      for: 5m
      labels:
        severity: warning
        component: data-plane
      annotations:
        summary: "High error rate for service"
        description: "Service {{ $labels.destination_service_name }} in namespace {{ $labels.destination_workload_namespace }} has {{ $value | humanizePercentage }} error rate (threshold: 5%)"

    - alert: HighP99Latency
      expr: |
        histogram_quantile(0.99,
          sum(rate(istio_request_duration_milliseconds_bucket{reporter="destination"}[5m])) by (destination_service_name, le)
        ) > 1000
      for: 10m
      labels:
        severity: warning
        component: data-plane
      annotations:
        summary: "High P99 latency detected"
        description: "Service {{ $labels.destination_service_name }} P99 latency is {{ $value }}ms (threshold: 1000ms)"

    - alert: ServiceUnavailable
      expr: |
        sum(rate(istio_requests_total{reporter="source", response_code="503"}[5m])) by (destination_service_name)
        > 0.1
      for: 2m
      labels:
        severity: critical
        component: data-plane
      annotations:
        summary: "Service returning 503 errors"
        description: "Service {{ $labels.destination_service_name }} is returning {{ $value }} 503 errors per second"

  # mTLS and Security Alerts
  - name: istio.security.rules
    interval: 30s
    rules:
    - alert: mTLSCertificateExpiringSoon
      expr: (envoy_server_days_until_first_cert_expiring * 24 * 3600) < 86400
      for: 1h
      labels:
        severity: warning
        component: security
      annotations:
        summary: "mTLS certificate expiring soon"
        description: "Certificate for pod {{ $labels.pod }} in namespace {{ $labels.namespace }} expires in less than 24 hours"

    - alert: mTLSCertificateExpired
      expr: envoy_server_days_until_first_cert_expiring < 0
      for: 5m
      labels:
        severity: critical
        component: security
      annotations:
        summary: "mTLS certificate has expired"
        description: "Certificate for pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has expired"

    - alert: InsecureConnectionsDetected
      expr: |
        sum(rate(istio_requests_total{reporter="destination", connection_security_policy!="mutual_tls"}[5m])) by (destination_service_name)
        > 0
      for: 5m
      labels:
        severity: critical
        component: security
      annotations:
        summary: "Non-mTLS connections detected"
        description: "Service {{ $labels.destination_service_name }} is receiving non-mTLS traffic: {{ $value }} requests/second"

  # Circuit Breaker and Resilience Alerts
  - name: istio.resilience.rules
    interval: 30s
    rules:
    - alert: CircuitBreakerTriggered
      expr: sum(rate(envoy_cluster_upstream_rq_pending_overflow[5m])) by (cluster_name) > 0
      for: 1m
      labels:
        severity: warning
        component: resilience
      annotations:
        summary: "Circuit breaker triggered"
        description: "Circuit breaker for cluster {{ $labels.cluster_name }} is being triggered at {{ $value }} requests/second"

    - alert: HighConnectionPoolOverflow
      expr: sum(rate(envoy_cluster_upstream_rq_pending_overflow[5m])) by (cluster_name) > 1
      for: 5m
      labels:
        severity: critical
        component: resilience
      annotations:
        summary: "High connection pool overflow"
        description: "Cluster {{ $labels.cluster_name }} connection pool is overflowing at {{ $value }} requests/second"

    - alert: HighOutlierDetectionEjections
      expr: sum(rate(envoy_cluster_outlier_detection_ejections_active[5m])) by (cluster_name) > 0.1
      for: 5m
      labels:
        severity: warning
        component: resilience
      annotations:
        summary: "High rate of outlier detection ejections"
        description: "Cluster {{ $labels.cluster_name }} has {{ $value }} endpoints being ejected per second"

    - alert: NoHealthyUpstream
      expr: envoy_cluster_membership_healthy == 0
      for: 1m
      labels:
        severity: critical
        component: resilience
      annotations:
        summary: "No healthy upstream endpoints"
        description: "Cluster {{ $labels.cluster_name }} has no healthy upstream endpoints"

  # Gateway Alerts
  - name: istio.gateway.rules
    interval: 30s
    rules:
    - alert: IngressGatewayDown
      expr: up{app="istio-ingressgateway"} == 0
      for: 5m
      labels:
        severity: critical
        component: gateway
      annotations:
        summary: "Ingress gateway is down"
        description: "Ingress gateway pod {{ $labels.pod }} has been down for more than 5 minutes"

    - alert: EgressGatewayDown
      expr: up{app="istio-egressgateway"} == 0
      for: 5m
      labels:
        severity: warning
        component: gateway
      annotations:
        summary: "Egress gateway is down"
        description: "Egress gateway pod {{ $labels.pod }} has been down for more than 5 minutes"

    - alert: GatewayHighErrorRate
      expr: |
        sum(rate(istio_requests_total{reporter="source", source_workload=~"istio-.*gateway", response_code=~"5.*"}[5m]))
        /
        sum(rate(istio_requests_total{reporter="source", source_workload=~"istio-.*gateway"}[5m]))
        > 0.05
      for: 5m
      labels:
        severity: critical
        component: gateway
      annotations:
        summary: "High gateway error rate"
        description: "Gateway {{ $labels.source_workload }} error rate is {{ $value | humanizePercentage }}"

  # Performance Alerts
  - name: istio.performance.rules
    interval: 30s
    rules:
    - alert: HighProxyResourceUsage
      expr: |
        sum(rate(container_cpu_usage_seconds_total{container="istio-proxy"}[5m])) by (pod, namespace)
        > 0.8
      for: 10m
      labels:
        severity: warning
        component: performance
      annotations:
        summary: "High proxy CPU usage"
        description: "Istio proxy in pod {{ $labels.pod }} (namespace: {{ $labels.namespace }}) CPU usage is above 80%"

    - alert: HighProxyMemoryUsage
      expr: |
        container_memory_usage_bytes{container="istio-proxy"}
        /
        container_spec_memory_limit_bytes{container="istio-proxy"}
        > 0.85
      for: 10m
      labels:
        severity: warning
        component: performance
      annotations:
        summary: "High proxy memory usage"
        description: "Istio proxy in pod {{ $labels.pod }} (namespace: {{ $labels.namespace }}) memory usage is above 85%"

    - alert: SlowConfigPropagation
      expr: histogram_quantile(0.99, rate(pilot_proxy_convergence_time_bucket[5m])) > 10
      for: 5m
      labels:
        severity: warning
        component: performance
      annotations:
        summary: "Slow configuration propagation"
        description: "P99 proxy convergence time is {{ $value }}s (threshold: 10s)"

---
# ServiceMonitor for Istio control plane
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: istio-component-monitor
  namespace: aikernel-monitoring
  labels:
    monitoring: istio-components
spec:
  selector:
    matchLabels:
      istio: pilot
  namespaceSelector:
    matchNames:
    - istio-system
  endpoints:
  - port: http-monitoring
    interval: 15s
    path: /metrics
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: namespace

---
# ServiceMonitor for Envoy sidecars
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: envoy-stats-monitor
  namespace: aikernel-monitoring
  labels:
    monitoring: istio-proxies
spec:
  selector:
    matchLabels:
      istio-prometheus-ignore: "false"
  namespaceSelector:
    any: true
  endpoints:
  - port: http-envoy-prom
    interval: 15s
    path: /stats/prometheus
    relabelings:
    - sourceLabels: [__meta_kubernetes_pod_container_name]
      action: keep
      regex: "istio-proxy"
    - sourceLabels: [__meta_kubernetes_pod_name]
      targetLabel: pod
    - sourceLabels: [__meta_kubernetes_namespace]
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_pod_label_app]
      targetLabel: app

---
# Grafana Dashboard ConfigMap - Control Plane Dashboard
apiVersion: v1
kind: ConfigMap
metadata:
  name: istio-control-plane-dashboard
  namespace: aikernel-monitoring
  labels:
    grafana_dashboard: "1"
data:
  istio-control-plane-dashboard.json: |
    {
      "annotations": {
        "list": [
          {
            "builtIn": 1,
            "datasource": "Prometheus",
            "enable": true,
            "hide": true,
            "iconColor": "rgba(0, 211, 255, 1)",
            "name": "Annotations & Alerts",
            "type": "dashboard"
          }
        ]
      },
      "editable": true,
      "gnetId": null,
      "graphTooltip": 0,
      "id": null,
      "links": [],
      "panels": [
        {
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "mappings": [],
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "red", "value": null},
                  {"color": "green", "value": 1}
                ]
              }
            }
          },
          "gridPos": {"h": 4, "w": 6, "x": 0, "y": 0},
          "id": 1,
          "options": {
            "colorMode": "background",
            "graphMode": "none",
            "justifyMode": "auto",
            "orientation": "auto",
            "reduceOptions": {
              "calcs": ["lastNotNull"],
              "fields": "",
              "values": false
            }
          },
          "targets": [
            {
              "expr": "up{job=\"pilot\"}",
              "refId": "A"
            }
          ],
          "title": "Control Plane Status",
          "type": "stat"
        },
        {
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "mappings": [],
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "green", "value": null}
                ]
              },
              "unit": "short"
            }
          },
          "gridPos": {"h": 4, "w": 6, "x": 6, "y": 0},
          "id": 2,
          "options": {
            "colorMode": "value",
            "graphMode": "area",
            "justifyMode": "auto",
            "orientation": "auto",
            "reduceOptions": {
              "calcs": ["lastNotNull"],
              "fields": "",
              "values": false
            }
          },
          "targets": [
            {
              "expr": "sum(envoy_cluster_membership_total)",
              "refId": "A"
            }
          ],
          "title": "Total Endpoints",
          "type": "stat"
        },
        {
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "mappings": [],
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "green", "value": null},
                  {"color": "yellow", "value": 80},
                  {"color": "red", "value": 90}
                ]
              },
              "unit": "percent"
            }
          },
          "gridPos": {"h": 4, "w": 6, "x": 12, "y": 0},
          "id": 3,
          "options": {
            "colorMode": "value",
            "graphMode": "area",
            "justifyMode": "auto",
            "orientation": "auto",
            "reduceOptions": {
              "calcs": ["lastNotNull"],
              "fields": "",
              "values": false
            }
          },
          "targets": [
            {
              "expr": "100 * (1 - sum(rate(istio_requests_total{response_code=~\"5.*\"}[5m])) / sum(rate(istio_requests_total[5m])))",
              "refId": "A"
            }
          ],
          "title": "Global Success Rate",
          "type": "stat"
        },
        {
          "datasource": "Prometheus",
          "fieldConfig": {
            "defaults": {
              "mappings": [],
              "thresholds": {
                "mode": "absolute",
                "steps": [
                  {"color": "green", "value": null}
                ]
              },
              "unit": "reqps"
            }
          },
          "gridPos": {"h": 4, "w": 6, "x": 18, "y": 0},
          "id": 4,
          "options": {
            "colorMode": "value",
            "graphMode": "area",
            "justifyMode": "auto",
            "orientation": "auto",
            "reduceOptions": {
              "calcs": ["lastNotNull"],
              "fields": "",
              "values": false
            }
          },
          "targets": [
            {
              "expr": "round(sum(rate(istio_requests_total{reporter=\"source\"}[1m])), 0.001)",
              "refId": "A"
            }
          ],
          "title": "Global Request Rate",
          "type": "stat"
        }
      ],
      "schemaVersion": 27,
      "style": "dark",
      "tags": ["istio", "control-plane"],
      "templating": {"list": []},
      "time": {"from": "now-1h", "to": "now"},
      "timepicker": {},
      "timezone": "",
      "title": "Istio Control Plane Dashboard",
      "uid": "istio-control-plane",
      "version": 0
    }

---
# AlertManager configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: aikernel-monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m

    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      routes:
      - match:
          severity: critical
        receiver: 'critical'
        continue: true
      - match:
          severity: warning
        receiver: 'warning'

    receivers:
    - name: 'default'
      webhook_configs:
      - url: 'http://alertmanager-webhook-receiver:8080/alerts'
        send_resolved: true

    - name: 'critical'
      webhook_configs:
      - url: 'http://alertmanager-webhook-receiver:8080/critical'
        send_resolved: true

    - name: 'warning'
      webhook_configs:
      - url: 'http://alertmanager-webhook-receiver:8080/warning'
        send_resolved: true

    inhibit_rules:
    - source_match:
        severity: 'critical'
      target_match:
        severity: 'warning'
      equal: ['alertname', 'cluster', 'service']
